{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<H1>MNIST</H1>\n* Reconhecimento de digitos\n* 28x28 (50k p/ treino e 10k p/ teste)\n* 0-9 (10 classes)"},{"metadata":{"trusted":true,"_uuid":"11c2c69185f1582b9f785792835db49320c83cef"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/fashionmnist/fashion-mnist_train.csv\")\ntest_df = pd.read_csv(\"../input/fashionmnist/fashion-mnist_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e93a4407522039560b6d2fec9f59a9f5e69c8a71","collapsed":true},"cell_type":"code","source":"train_data = train_df.get_values()\ntest_data = test_df.get_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2945f925e3de1b98b4b2c43511173fc3deb14df","collapsed":true},"cell_type":"code","source":"train_imgs = train_data[:, 1:]\ntrain_labels = train_data[:, 0]\n\ntest_imgs = test_data[:, 1:]\ntest_labels = test_data[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"982978a947ce83879bb635526a96f62f36afdff3"},"cell_type":"code","source":"train_imgs.shape, train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ceda2cd79f54f4c5d4a354acec07a6990963f4d","collapsed":true},"cell_type":"code","source":"train_imgs = train_imgs.reshape((60000, 28, 28, 1))\ntest_imgs = test_imgs.reshape((10000, 28, 28, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5071b0b7c713339e75ee1de2c6ac414372c5d85"},"cell_type":"code","source":"train_imgs.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b564c1d3446bc005a499a2b1db0cdbcc5099f260"},"cell_type":"markdown","source":"<h2>visualizando os dados</h2>"},{"metadata":{"_uuid":"b5a02aeb6b3b54d9142ca17d598b05934cf8bdb7","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n%matplotlib inline\n\nplt.imshow(train_imgs[2,:, :,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b6d405b879cdb4bda834b751d322d9d692ab3d"},"cell_type":"code","source":"from keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56f5d81ed0f8e89c1b552f859f8538c2fb6b8393","collapsed":true},"cell_type":"code","source":"train_labels = to_categorical(train_labels, num_classes=10)\ntest_labels = to_categorical(test_labels, num_classes=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddf12e117e33950debc3708f1f105d65f006b634"},"cell_type":"markdown","source":"<h2>Construindo a rede</h2>"},{"metadata":{"trusted":true,"_uuid":"a7e42b5dca48b8139a0e60a94aa2f96e0b42fc75","collapsed":true},"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c854e3440b4b27cfd7ac82ff9582cc4122e6a91e"},"cell_type":"markdown","source":"VAMOS UTILIZAR OS SEGUINTES PARAMETROS:\n* 32 filtros\n* 3 x 3 Ã© o tamanho do filtro\n* stride 1"},{"metadata":{"trusted":true,"_uuid":"0293f54f5be758445ceac893c4e6ddc7483bdaec"},"cell_type":"code","source":"input_node = Input(shape=(28, 28, 1))\n\nconv1 = Conv2D(32, (3, 3), strides=1, padding='same', activation='relu' )(input_node)\npool1 = MaxPooling2D((2,2), strides=(2,2))(conv1)\n\nconv2 = Conv2D(64, (3, 3), strides=1, padding='same', activation='relu' )(pool1)\npool2 = MaxPooling2D((2,2), strides=(2,2))(conv2)\n\nconv3 = Conv2D(128, (3, 3), strides=1, padding='same', activation='relu' )(pool2)\npool3 = MaxPooling2D((2,2), strides=(2,2), padding='same')(conv3)\n\nflat = Flatten()(pool3)\nout = Dense(10, activation='softmax')(flat)\n\nmodel = Model(input_node, out)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"520f01dd77bcaf39d3b1bd4474ecd0cfb118c540","collapsed":true},"cell_type":"code","source":"model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d5868e60710635612f55e6fc21e004dec451872","collapsed":true},"cell_type":"code","source":"from keras.callbacks import *\n\nfolder = 'logs/'\nif not os.path.isdir(folder):\n    os.makedirs(folder)\n\n\nmodel_checkpoint = ModelCheckpoint(monitor='val_loss', \n                                   filepath=folder+'model{epoch:02d}.{loss:2.4f}.{acc:2.2f}.{val_loss:2.4f}.{val_acc:2.2f}.hdf5')\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=3)\n\ncallbacks = [model_checkpoint, early_stop]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ff1350e3970ddcde89b4440b5fd132f08a033ed"},"cell_type":"markdown","source":"<h3>Normalizando as imagens</h3>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2be25cc48d6bfa1be41dd029513e61072775cbed"},"cell_type":"code","source":"train_imgs = train_imgs / 255\ntest_imgs = test_imgs / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b3440e47718ea62ad08edf902563c5b5e40bd2a"},"cell_type":"code","source":"model.fit(train_imgs, train_labels, epochs=20, \n          validation_data=(test_imgs, test_labels), \n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26aa77a0781cf007bf1281cfd89d4c5cde409590"},"cell_type":"markdown","source":"<h2>Testando imagem real</h2>"},{"metadata":{"trusted":true,"_uuid":"a4a397e3c9e8a27ced775e1bdb6b34595cadf2ae"},"cell_type":"code","source":"import cv2\n\ntest_img = cv2.imread('../input/vestidopreto/vestido-preto.jpg', 0)\nplt.imshow(test_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe28d324f7182e4af87acc756271c6cedd0ac6b3"},"cell_type":"code","source":"test_img = cv2.resize(test_img, (28, 28))\nplt.imshow(test_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1a8e7aca35577cdfe4b7d3ffceea48e31c504579"},"cell_type":"code","source":"test_img = test_img.reshape((28,28, 1))\ntest_img = np.expand_dims(test_img, 0)\ntest_img = test_img / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1799d876932d0beea18c4abda3e8dfff21e304e"},"cell_type":"code","source":"np.round(model.predict(test_img), 3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}